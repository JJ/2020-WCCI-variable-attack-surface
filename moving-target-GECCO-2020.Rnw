%\documentclass[sigconf, authordraft]{acmart}
\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

% ISBN
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}

% Conference
\acmConference[GECCO '20]{the Genetic and Evolutionary Computation Conference 2020}{July 8--12, 2020}{Cancun, Mexico}
\acmYear{2020}
\copyrightyear{2020}

%\acmArticle{4}
\acmPrice{15.00}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\begin{document}


<<setup, cache=FALSE,echo=FALSE>>=
suppressPackageStartupMessages({
    library(ggplot2)
    library(ggthemes)
})
active.fitness.data <- read.csv("results/initial-fitness.dat")
experiments.data <- read.csv("results/all_results.csv")
@

\title{Moving target defense through evolutionary algorithms}

%%% The submitted version for review should be ANONYMOUS
\author{Ben Trovato}
\authornote{Dr.~Trovato insisted his name be first.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin} 
  \state{Ohio} 
  \postcode{43017-6221}
}
\email{trovato@corporation.com}

\author{G.K.M. Tobin}
\authornote{The secretary disavows any knowledge of this author's actions.}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin} 
  \state{Ohio} 
  \postcode{43017-6221}
}
\email{webmaster@marysville-ohio.com}

\author{Lars Th{\o}rv{\"a}ld}
\authornote{This author is the
  one who did all the really hard work.}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla} 
  \country{Iceland}}
\email{larst@affiliation.org}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{B. Trovato et al.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Moving target defense is a technique for protecting internet-facing
systems via the creation of a variable attack surface. In the case of
internet servers, it can be achieved via the generation of different
configurations that change the server profile, and that can be
included in a policy of restarting services with new configurations
after a random time and with a random frequency. In this paper we will
present a method based on evolutionary algorithm, that uses
industry-standard practices to score the vulnerability of a server and
is designed to generate multiple configurations with optimized score
in every run of the algorithm. In this paper we will present how this
is achieved via tuning of the evolutionary algorithm with the
challenge of the very costly fitness evaluation that only allows for a
very limited evaluation budget.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002978.10003006.10011634.10011635</concept_id>
<concept_desc>Security and privacy~Vulnerability scanners</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003809.10003716.10011136.10011797.10011799</concept_id>
<concept_desc>Theory of computation~Evolutionary algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010521.10010537.10003100</concept_id>
<concept_desc>Computer systems organization~Cloud computing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Vulnerability scanners}
\ccsdesc[500]{Theory of computation~Evolutionary algorithms}
\ccsdesc[300]{Computer systems organization~Cloud computing}

\keywords{Security, cyberattacks, performance evaluation, moving
  target defense, evolutionary algorithms, cloud computing, CVSS}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Cybersecurity is a discipline that taps from all corners of computer
science, including optimization in many cases. Many security problems
can be formulated as a search problem, as long as the vulnerability of
a system (along with any other objective we want to achieve, such as
throughput or performance) can be given a fixed score.

Defending targets against attacks includes many different
techniques. Of course, hardening is one of them, and this is a problem
that can effectively be formulated as an optimization one: minimizing
the vulnerability. A hardened objective might still be attacked, as
long as the person attacking is able to profile it and try different
things on it, simultaneously or later on. Avoiding this is the main
objetive of the so callec Moving Target Defense, or MTD
\cite{moving-target,jajodia2011moving,Cai2016}.

This kind of defense does not specify either the kind of attack, the
service is protecting or the technique, it is a framework to define a
series of policies where the essence is to change configuration of the
service defended in that way after a random amount of time so that it
creates different traffic and response patterns that makes impossible
to identify a node that does a specific service in the network. In
this sense, we have chosen to work on defending web services (or web
sites) by optimizing the configuration of a web server, following the
work of \cite{john_evolutionary_2014}, which was one of the first
evolutionary approaches to this kind of methodology; unlike he did, we
have chosen the {\sf nginx} static web server and proxy, while he
focused on the well known Apache web server. Our chose was motivated
by the fact that nowadays {\sf nginx} is more popular, and still
growing in popularity nowadays.

In our previous papers \cite{erseco:evostar:anon,erseco:cec}, we realized that one of the main challenges
was to define a fitness function that would be able to reflect changes
in the vulnerabilities exposed by the site, even subtle ones. The
fitness function we used seemed reasonable, and was simply based in
counting the number of alerts raised by ZAP. While this is reasonable,
there are several problems with this, but from the point of view of an
evolutionary algorithm with a very limited budget, the main one is
that most changes in the chromosome will not imply a change in the
fitness; most importantly, a change in the severity of a particular
vulnerability will not be counted as such. This paper will be mainly
devoted to examine a new fitness function, that really takes into
account the severity of the vulnerability raised. We will try to prove
that this new fitness function improves the exploitation capabilities
of the algorithm, obtains more secure configurations, and is also able
to obtain many more low-vulnerability {\sf nginx} configurations in a
single run.

Additionally, we have made several piecewise improvements to our
previous system; we have debugged the configuration extensively to
ensure they produced only valid values; at the same time, we have
eliminated invalid values that were introduced originally for testing
purposes. We have also performed an analysis of the vulnerabilities
reported, in order to check what is the actual impact of configuration
changes on them; finally, we have made a small change to the selection
method to increase the variability of individuals that will be used
eventually for the variable configuration.

The rest of the paper is organized as follows: next we will present
the state of the art in evolutionary methods applied to MTD; the next
Section \ref{sec:met} will present the methodology used in this paper,
followed by the experimental results, which we will discuss in Section \ref{sec:met}
finishing with our conclusions and future lines of work.

\section{State of the art}
\label{sec:soa}


% And also rewrite as much of this as possible -------------------------------------

% Maybe make a big summary out of this part, which is focused on MTD
% -------------------------------------------------------------------
% MTD was proposed by the first time in 2009 \cite{moving-target} by an
% organism called NITRD as part of an officially sponsored research
% program to improve the cyberdefense skills in the United States. MTD is targeted 
% towards making what is called the attack surface
% \cite{manadhata2011formal}, that is, the different mechanisms by which
% the attacker would be able to gain access, unpredictable
% \cite{jajodia2011moving}, and thus rendering attacks against it either too
% expensive or too complex to pursue, possibly forcing the attacker to
% choose some other, more affordable, place. For instance, an attacker
% analyzing byte patterns coming from different nodes such as the one
% described in \cite{piskozub2019resilience} will find those patterns
% disrupted, and so profiling of specific nodes impossible.


% This program was pursued using different kind of techniques, of which
% a good initial survey was made in \cite{Cai2016}, reexamined in
% \cite{Larsen201428} and more recently in
% \cite{lei2018moving,ward2018survey,cho2019toward}. MTD is used as a
% defense as well as detection technique
% \cite{tian2019moving,POTTEIGER2020102954}; for instance, it can be
% used to deflect distributed denial of service attacks
% \cite{prathyusha2020review}; besides, it has been proved effective
% against exfiltration techniques via the use of an open source
% framework called MoonRaker \cite{shade2020moonraker} or to protect
% software defined networks \cite{al2011toward}. Several techniques have
% been applied recently; for instance, natural randomization in services
% can be enhanced \cite{kansal2020improving}; or, beyond the technique
% that is used, deep reinforcement learning can try and find the best
% moment for changing configurations \cite{eghtesad2019deep}, a topic
% that is normally left behind. These techniques have been surveyed in
% \cite{Zheng2019207,cho2019toward}, to which we direct the interested
% reader.

% However, in this paper we focus on those that use evolutionary
% algorithms as a method of optimization as well as generation of new
% configurations; evolutionary algorithms are no strangers in the
% cybersecurity world, and in fact, since early on, they were applied to intrusion
% detection systems \cite{WU20101}. It was only natural that they were
% also applied, since the inception of the technique, to MTD. An
% evolutionary-like bioinspired algorithm  called {\em
%   symbiotic embedded machines} (SEM) was proposed by Cui and Stolfo
% \cite{cui2011symbiotes} as a methodology for {\em injecting} code into
% systems that would behave in a way that would be similar to a
% symbiotically-induced immune system. Besides that principled
% biological inspiration, SEMs used mutation as a mechanism for avoiding
% signature based detection methods and thus become a MTD system.

% Other early MTD solutions included the use of rotating virtual webservers
% \cite{huang2011introducing}, every one with a different attack
% surface, to avoid predictability and achieve the variable attack
% surface that was being sought. However, while this was a practical and
% actionable kind of defense,
% no specific technique was proposed to individually configure every
% virtual server, proposing instead manual configuration of web servers
% (such as nginx and Apache), combined with plug-ins (some of which do
% not actually work together). A similar technique, taken to the cloud, was proposed
% by Peng et al. \cite{peng2014moving}:  a specific
% mechanism that uses different cloud instances and procedures for moving
% virtual machines between them; still, no other
% mechanism was proposed to establish these configurations, which were
% simply left to being designed by hand, as long as there were enough of them.


% And expand a lot this part, which is focused in application of evolutionary algorithms to this technique - JJ
% ---------------------------------------------------------------------------------------------------------------
Moving target defense is a very popular technique, with dozens of
papers published every year, including a yearly conference, the
Workshop on Moving Target Defense.

We will focus on bioinspired approaches to this field; a non-afiliated
bioinsired technique, related to immune systems, was among the first
ones to be proposed; it was denominated  {\em symbiotic embedded
  machines} (SEM) and was proposed by Cui and Stolfo
\cite{cui2011symbiotes}; after that one, explicit
methodologies that used evolutionary algorithms were conceptually described for the first
time by Crouse and Fulp in \cite{6111663}. This was intended mainly as
a proof of concept, and describes 80 parameters, of which just half
are evolved. The GA minimizes the number of vulnerabilities, but the
study also emphasizes the degree of diversity achieved by successive
generations in the GA, which impact on the diversity needed by the
MTD. Lucas et al. in \cite{lucas2014initial} applied those theoretical
concepts to a framework called EAMT, a Python-based system that uses
evolutionary algorithms to create new configurations, which are then
implemented in a virtual machine and scored using scanning tools such
as Nessus.

We will focus specially on the work by John et
al. \cite{john_evolutionary_2014}, which was also presented at a GECCO
workshop in 2014. John and coauthors make a more explicit and practical
use of an evolutionary algorithm, describing a host-level
defense system, that is, one that operates at the level of a single
node in the network, evolving the configuration of the Apache web
server using the so called CVSS score \cite{cvss}, a score that is computed from a
the categories of vulnerabilities detected and how severe they
are. This is a rich way of quantifying the vulnerability of a system,
and also gives different ways of improving security; by reducing the
number of vulnerabilities or by reducing its severity; this creates a
rich fitness landscape that the evolutionary algorithm can leverage to
find the best solutions.

This work highlighted the need for, first, a practical way of applying the MTD
to an actual system, to the point of implementing it in a real virtual
machine, and second, the problematic of scoring the generated
configurations. In the next section we will explain our proposed
solutions to these two problems.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology, experimental setup and results}
\label{sec:met}

% Maybe rewrite a part of this, expanding if possible - JJ

As in our previous paper \cite{erseco:evostar:anon}, we have chosen
{\sf nginx}; it's a very popular static web server, which is also used
as an inverse proxy, API gateway and load balancer. Latest versions
of{\sf nginx} (1.17.x) have more than 700 configuration
directives. As a matter of fact, {\sf nginx} has been the target of
optimization by evolutionary algorithms recently \cite{chi2018hybrid},
but this is not the main focus of our work presently.


These directives affect in different ways the behavior of
the web site (or service) that is behind it, or simply change the
values in the response headers; we will show the ones we will working
with next. The following Subsection \ref{subs:setup}
will outline the setup actually used for running the experiments, and
results will be presented last in Subsection \ref{subs:results}.

\subsection{Description of the attack surface parameters}

From all the parameters used to change the behavior of {\sf nginx}, we
have used fifteen, the same as in out previous paper
\cite{erseco:evostar:anon}, from where this section is largely taken
and expanded.

This subset is extracted from the DISA STIG recommendations for
hardening webservers based in the CVSS score
\cite{disa20:apache}. Most of these values are defined in the original
document as Apache HTTP server configuration values but have a {\sf
nginx} equivalent directive. All configuration parameters used are
{\sf nginx} directives, with nine of them, shown in Table
\ref{table:nginx_directives}, affecting the global behavior of the
server, and six affecting the values of specific HTTP headers (shown
in Table \ref{table:http_headers}), which in turn affect how the
browser receiving the information deals with the pages and the
infornation contained in them.

\begin{table}
\centering
\begin{tabular}{|l|l|c|}
\hline
\textbf{STIG ID} & \textbf{Directive name} 	   & \textbf{Possible values} \\ \hline
V-13730 & worker\_connections            & 512 - 2048 \\ \hline
V-13726 & keepalive\_timeout             & 10 - 120 \\ \hline
V-13732 & disable\_symlinks              & True/False \\ \hline
V-13735 & autoindex                      & True/False \\ \hline
V-13724 & send\_timeout                  & True/False \\ \hline
V-13738 & large\_client\_header\_buffers & 512 - 2048 \\ \hline
V-13736 & client\_max\_body\_size        & 512 - 2048 \\ \hline
V-6724  & server\_tokens                 & True/False \\ \hline
        & gzip                           & True/False \\ \hline
\end{tabular}
\caption{List of {\sf nginx} directives whose value is evolved here.}
 \label{table:nginx_directives}
\end{table}
%

These are the {\sf nginx} directives that have been used in this paper; their
equivalent STIG ID is shown in Table
\ref{table:nginx_directives} along with the range of values for
generation and ulterior evolution:\begin{itemize}
\item \texttt{worker\_connections}: Maximum number of simultaneous connections that can be opened by an {\sf nginx} process.
\item \texttt{keepalive\_timeout}: Period during which a server-side client
  connection will remain open  before timing out.
\item \texttt{disable\_symlinks}: Restricts opening files that are
  actually symbolic links. When set to off (the default), if  some component of the
  path is a symbolic link the access to that file is denied. An
  additional value, {\tt if\_not\_owner}, is not considered in this study.
\item \texttt{autoindex}: When activated it shows the contents of the directories, otherwise it does not show anything.
\item \texttt{send\_timeout}: sets a value for the waiting time to
  transmit a response to the client; the client will close the
  connection if nothing is received during that time. It is set by
  default to 60s, we will disable it completely (setting it to 0) or
  set it to 1 second.
\item \texttt{large\_client\_header\_buffers}: This directive has two
  values: the number of buffers that will read large client request
  headers, and their size. We just evolve the second number, while the
  first one is set to 4.

\item
\texttt{client\_max\_body\_size}: Maximum allowed size of the client request body, specified in the {\tt
  Content-Length} field of the request header. The value will indicate
the maximum number of Megabytes allowed. Over that value, requests
will simply return an error.

\item \texttt{server\_tokens}: Enable or disable the broadcast of the {\sf
  nginx} version on the error pages and in the {\tt Server} response
header. It's on by default, and we admit on or off values, although the
directive can also use a {\em build} value and simply a string.
\item
\texttt{gzip}: This directive enables or disables compression of HTTP
responses. This directive doesn't affect directly the security but
adds entropy to the different generated configurations; it also
affects the performance of the server and its throughput. You can
additionally use {\tt gzip\_min\_length} to establish the length under
which there will be no compression.
\end{itemize}

\begin{table}

  \centering
  \caption{Selected list of directives affecting HTTP headers, and the
  values that we are using in this paper. \label{table:http_headers}}
 

\begin{tabular}{|l|l|}
\hline
\textbf{Header name}           & \textbf{Possible values} \\ \hline
X-Frame-Options                & \shortstack[l]{SAMEORIGIN \\
  ALLOW-FROM \\ DENY \\ WRONG VALUE} \\ \hline
X-Powered-By                   & \shortstack[l]{PHP/5.3.3 \\ PHP/5.6.8 \\ PHP/7.2.1 \\ Django2.2 \\ nginx/1.16.0} \\ \hline
X-Content-Type-Options         & nosniff \\ \hline
Server                         & \shortstack[l]{apache \\ caddy \\ nginx/1.16.0} \\ \hline
X-XSS-Protection	       & \shortstack[l]{0 \\ 1 \\ 1; mode=block} \\ \hline
Content-Security-Policy	       & \shortstack[l]{default-src 'self' \\ default-src 'none' \\ default-src 'host *.google.com'} \\ \hline
\end{tabular}

\end{table}
%


With every web page, the HTTP response from the server includes a
number of headers; part of them are automatically added by the server
or have a default value, while others can be configured; these headers
sometimes simply add metadata (like the system date, for instance),
while others have a precise value that is interpreted by the browser,
and thus can have security implications; these headers, which usually
are preceded by {\tt X}, are also usually {\em de facto} standards,
with certain standard values. The ones we have chosen are explained
below, and the values they can take in this research represented in
Table \ref{table:http_headers}.
\begin{itemize}
\item
\texttt{X-Frame-Options}: The header that has the same name as this
directive, when set, bans browsers rendering of frame-embedded pages. Web servers can use it to prevent what are called
\textit{clickjacking} attacks on their pages, that is, overlaying a
transparent frame over a site to make users click on the transparent
layer believing they are clicking on the one below. Several values are
possible: only allow if the embedding page belongs to the same site,
for instance, but you can also directly ban it. We have 4 different
options for this, including a value that's intentionally wrong.
\item
\texttt{X-Powered-By}: This is a string that is supposed to tell the
name and versions of the application that generated the response. It
is generally recommended not giving too extensive information in this
header because can reveal details that can facilitate the task of
finding and exploiting security flaws for specific versions just
looking up in an online database. Setting different values do not affect directly to the
security by itself but adds entropy to the generated configurations.
\item
\texttt{X-Content-Type-Options}: This directive controls the operation
of the homonym HTTP response header; when set,  changing the \textit{MIME} types
announced in the `Content-Type' is disabled, which is used to avoid
`MIME type sniffing' attacks; that is, using what browser's heuristics
for detecting MIME type from content to launch a cross-site scripting
attack. 
\item
\texttt{server}: This directive is related to the {\tt Server} HTTP
header, which is used to communicate to browsers metadata about the
server software used by the application. This can be as informative or
as misleading as we want; as a matter of fact, it is a good practice
not to give too
extensive information of software versions, but we can cheat the
attacker telling wrong server version info. Doesn't affect directly to
the security but adds entropy to the generated configurations. This
directive, along with {\tt X-Powered-By}, are mainly used for
informative or statistics purposes and do not really change anything
either in content or how it is rendered by the server.
\item
\texttt{X-XSS-Protection}: This directive is used to set the value of
the corresponding HTTP response header; as the rest of these server
directives, it's interpreted by browsers and used by them to avoid
loading pages when they detect reflected cross-site
scripting (XSS) attacks. % Need to clarify this.
\item
\texttt{Content-Security-Policy}: we can set this directive to
different values to avoid the load of certain kind of content. We have
three different values for this directive, allowing content only if
it's loaded from the same page ({\tt self}), loading from nowhere
({\tt none}), or including Google domains (with the directive {\tt hosts:*.google.com}), in
this case mainly for user tracking. These values are shown in Table
\end{itemize}

The problem of optimizing security in configuration is twofold: some
of these directives are security-neutral, and do not actually alter
the security score, they simply add to overall byte-level diversity;
some of them, by themselves, will make a site more secure; in some
other cases, it will be its combination what makes it safer. That is
why a global optimization algorithm is needed to get variable, and
also secure, attack surface. Eventually, a {\sf nginx} configuration
file such as this one will be generated:

\begin{verbatim}
user nginx;
pid /var/run/nginx.pid;
worker_processes 4;
daemon on;
error_log /tmp/nginx-error.log warn;
events {
    worker_connections 667; #
}
http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    access_log /tmp/nginx-access.log;
    sendfile on;
    keepalive_timeout 46; #
    disable_symlinks off; #
    autoindex off; #
    send_timeout 1; #
    large_client_header_buffers 4 804; #
    client_max_body_size 1396736; #
    server_tokens off; #
    gzip off; #
    log_format my_tracking $request_body;
    resolver 8.8.8.8 valid=30s;
    server {
        server_name www.exampletfm.com;
        listen 80;
        error_page 500 502 503 504 /50x.html;
        location ^~ /assets/public/assets/ {
            deny all;
        }
        location ^~ /assets/assets/ {
            deny all;
        }
        location /form {
            access_log /tmp/access.log my_tracking;
        }
        location / {
            root /tester/site/;
            index index.html index.htm;
            add_header X-Frame-Options DENY; # 
            add_header X-Powered-By PHP/7.2.1; #
            add_header X-Content-Type-Options nosniff; #
            add_header Server caddy; #
            add_header X-XSS-Protection "1; mode=block"; #
            add_header Content-Security-Policy "default-src 'self'; frame-ancestors 'self';";#
        }
    }
}
\end{verbatim}

The fifteen parameters that have been generated by the evolutionary
algorithm are marked with a hash mark \#. The upper block are global
directives, the lower block affects only the root location, leaving
other subdirectories unaffected.

% Maybe add here an analysis of how different directives affect
% fitness.

\subsection{Experimental setup}
\label{subs:setup}

The most important part of the evolutionary algorithm is designing
correctly the fitness function that is going to be tested and used.
A configuration is meaningless without
content behind, and we need to choose what is going to be the
content. As in our previous paper \cite{erseco:evostar:anon}, a basic
application with just a few pages and a form, was created, and
additionally, a intentionally vulnerable application, OWASP's Juice
Shop \cite{juice-shop}, was also tested.
Since their vulnerabilities are different, the
scores are going to be different.


The setup used for computing this score is exactly the same as in the
last paper: standard Docker containers for the juice shop and the ZAP
API were composed (using Docker Compose) together with the container
hosting the evolutionary algorithm, which calls from the fitness
function (written in Python) the ZAP library. What ZAP does is to run
(spider) 
over all pages in the site, making different attacks and raising
alerts if they detect any vulnerability. For every vulnerability
found, it raises an alert. In our previous paper we simply used the
number of alerts as a fitness score (to be minimized); while being
adequate for a proof of concept, it does not actually reflect the
state of vulnerability created by the configuration under evaluation,
mainly because these alerts have different nature.

ZAP classifies alerts in four different classes, depending on its
severity: High, Medium, Low and Informational. Of these kind of
vulnerabilities, Informational can be dismissed, since they are simply
notes about some best practice not being followed strictly. For
instance, one such alert could be:

\begin{verbatim}
The response appears to contain suspicious comments which may help an
attacker.
\end{verbatim}

We decided to work with the rest to assign a vulnerability score to
every configuration: Low will be scored with 1, Medium will get 2 as
score, and High will get a 4 score.

This score is to be minimized, but a ``good'' configuration will be
one that does not have any ``Medium''-scored alert. 

Evaluation of every configuration is carried out in the following way:\begin{itemize}
  
\item A new configuration is generated from the evolutionary algorithm
  values and stored in a configuration file with random name. This has
  been introduced in this version, and makes sure that every
  configuration is evaluated only once.

\item This configuration is checked for correctness, and 999 returned
  as score if it's incorrect.
\item The program ensures the previous instance of the server has been
  killed, and starts up the server.
\item ZAP scans the site
\item The program kills the server, checking several times until it's gone

\end{itemize}

Scanning is the part of this process that takes the most time, since
it implies making requests to every single page in the system and
analyzing the response. In this version of the tester we have
increased the number of maximum workers the {\sf nginx} server is able
to process, which might speed it up a bit. Still, a single run takes
several hours, leaving us with just a few possible experiments to
conform, and every one of them with just a few evaluations, much less
that are standard in the industry.

% More explanations on current version - JJ

After testing different types of crossover and mutation in our
previous paper \cite{erseco:evostar:anon} in this paper we have tried to
simplify; those experiments resulted in any kind of mutation and
crossover being approximately as effective as the next one. However,
mutation used was too disruptive, and didn't allow for gradual change
and exploration of the configuration space, so in this case we have
used a {\em incremental} mutation, that adds or subtracts one from
current values, and circles back to the opposite end of the range if
these are exceeded; thus, if the value of the {\tt disable\_symlinks}
element is 1, it will become 0 no matter what (either if it's added
one or subtracted one); if the value of {\tt worker\_connections} is
2048 it will become 512 if it's added one. This guarantees that every
mutation will result in a different value; also, only one randomly
chosen value is changed every time.

Crossover will be 2-point crossover, and it will return a single
value, with pieces taken from both parents.

While in the previous paper we were using a simple rank-based,
non-fitness-proportional selection, which probably resulted in less
exploitation of the best results, which is why in this paper we have
changed that to 2-tournament selection.

All together, every generation, a reproduction pool of the same size
as the population is created; these are randomly picked in couples,
crossed and the result mutated, creating a new set of individuals;
this new set of individuals is evaluated; the result is merged with
the previous population, with just the best ones selected to pass to
the next generation.


This evolutionary algorithm should offer better results than the
previous naive one, so we explored its result by making some test runs
using 32 individuals and 32 generations, double the number of
generations we had used in our previous paper. How the fitness score
evolves along all the generations is shown in Figure
\ref{fig:evolution}, which is an overview on how evolution
proceeds. It shows many plateaus, first at score equal to 68 and then
a big plateau for value = 62, to a point in which all the population
has that score. However, exploration proceeds apace and eventually we
obtain a ZAP score of 59 by the end of the evaluation budget.

From this initial exploration of values, we can conclude that even a
small number of evaluations (only 1024) is able to obtain good
results, and that the evolutionary algorithm is able to overcome, at
least in some cases, plateaus with low fitness diversity across all
the population. It is also evident that the evaluation budget is not
enough and that more generations could be used to obtain better values
of this score, down to 3 which seems to be the absolute minimum for
the Juice Shop. This initial exploration took 6 hours in a Lenovo
Carbon X5 laptop with a i7 CPU and Ubuntu 16.04, which also gave us an
idea of the time we were going to need to devote to these
experiments. The results of these will be shown next.

\subsection{Experimental results}
\label{subs:results}

% Show average results and how many of the same value are generated in
% the last generation.

We performed several runs for population 16 and population 32, in
every case with 32 generations. The main objective of these runs was
not so much to measure the final result, since there are not so many
evaluations, but to evaluate in which measure the evolutionary
algorithm contributed to the improvement of the score of the generated
configurations, as well as how many configurations, in the last
generation, had the best score. We will examine individual results,
shown in table \ref{tab:experiments}.
%
\begin{table*}
\centering
\caption{Experiment results for every run made for population 16 and
  32. ``Copies'' indicates the fraction of the population whose value
  is the same as the best individual.}
\label{tab:experiments}
<<experiments, cache=FALSE,echo=FALSE>>=
kable(experiments.data)
@
\end{table*}
%
Experiments with population = 16 took around 6 hours in an Amazon EC2
instance, while experiments with population = 32 took twice as much;
this is the main reason why no more results are shown. In practice,
moving target defense would change configuration every few hours,
which makes these results acceptable for its purpose, although it
obviously would admit a certain degree of improvement.

The results in which population is only 16 evidence that what it
essentially does is to generate different configurations with the best
fitness found originally in the population: the final population is
filled with mutated copies of a configuration, all of which have the
same fitness. It happens to be 12 in these cases, which is a low ZAP
score, but in the case an element with that score wouldn't have been
in the initial population it would have been difficult to achieve that
value with just a few evaluations (512, in this case). However, this
result is acceptable, and shows that an evolutionary algorithm is
able, at least, to generate a good amount of diverse configuration,
even if at this population level it's not able to improve initial
scores, just to weed out invalid configurations, or simply those with
a low score.

The runs we were able to make with population = 32, and double the
amount of evaluations we did before, 1024, do show a lot of
improvement of initial configurations. In two cases, there's just one
configuration with the same value, but most of them have a ZAP score
below 62, which is a good value. In one case it was able to generate a
quarter of the population with the same ZAP score, 53, but also a few
more with values 54, 58 and 59, and all of them below 62, eliminating
in any case all invalid configurations from the population. In all
cases average is around 58, which is quite an improvement over initial
averages, which are high mainly due to the presence of invalid
configurations.

At any case, these results prove that improving the evolutionary
algorithm makes our method able to extract many more valid
configurations that can be used in the movable target defense method,
and is able to do so in a reasonable amount of time; compared to our
previous paper, the exploitation of values present in the initial
population is better, and we are also able to improve initial values
by some measure. 

All the results of experiments, as well as their code and the scripts
needed to generate this data can be found in
\url{https://github.com/JJ/2020-WCCI-variable-attack-surface}, and can
be reused with a free license.

\section{Conclusions and discussion}
\label{sec:conclusions}

% Write conclusions here


% Future lins of work.
In the immediate future our focus will be on analyzing the different
parts of the experimental setup so that we can iron out all problems,
and also attain whatever speed ups can be attained with it, so that
evaluation can be faster; for instance, we should try and look at a
way of evaluating several configurations in parallel, and maybe make
the algorithm asynchronous so that it does not have to wait for a full
generation before performing its operations. This is one of the main drawbacks of the algorithm right now,
and although part of it is inherent (it still has to try every single
page of a site), we could try to achieve faster
evaluation by using a different web for generating the configuration
and for deploying the configuration. Using the actual web we are
testing for evolving
configurations can be incredibly time-consuming; the use of surrogates
would speed up evolution, either by creating surrogates of the web
itself or by trying to make parts of the evaluation via surrogates
obtained a deep learning algorithm which, besides, should give us some
insight on the problems created by different features and what predict
insecure configurations just by looking at them, without the need to
put up an actual web site.

In this paper we have also found that the baseline secure
configuration has flaws that do not have anything to do with the
configuration of the web server. We will try to expand the
configuration to the content itself, and also to specific parts of
this content or how it has been generated. We can also extend
evolutionary MTD to other services, whose configurations can be
evolved alongside the web server for more variable attack surfaces, or
even change the web server so that the configuration is even more
difficult to detect. Adding multi-objective evolution so that we
improve performance alongside security is also something we intend to
pursue in the future.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}

Hidden for double-blind review
% This paper has been supported in part by projects DeepBio (TIN2017-85727-C4-2-P).

\bibliographystyle{ACM-Reference-Format}
\bibliography{geneura,moving-target}

\end{document}
