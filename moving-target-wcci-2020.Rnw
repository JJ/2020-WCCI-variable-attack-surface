\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}


\begin{document}


<<setup, cache=FALSE,echo=FALSE>>=
library(ggplot2)
library(ggthemes)
data <- read.csv("results/all_results.csv")
@

\title{Improving evolution of service configurations for moving target
defense}

\author{\IEEEauthorblockN{Ernesto Serrano-Collado}
\textit{University of Granada}\\
Granada, Spain \\
info@ernesto.es % Add your name here
\and
\IEEEauthorblockN{Juan J. Merelo-Guerv\'os}
\IEEEauthorblockA{\textit{Dept. of Computer Architecture and Technology} \\
\textit{University of Granada}\\
Granada, Spain \\
jmerelo@ugr.es}

}


\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The term {\em moving target defense} or MTD describes a series of
techniques that change the configuration of an Internet-facing system; in
general, the technique consists of changing the visible configuration to
avoid offering a fixed target to service profiling
techniques. Additionally, configurations need to be as secure as
possible and, since change needs to be frequent, to generate also as many as
possible. We previously introduced a proof of concept where we used
a simplified evolutionary algorithm for generating these configurations.
In this paper we improve this algorithm, trying to
adapt it to the specific characteristics of the fitness landscape, and
also looking at finding as many solutions as possible.
\end{abstract}
\begin{IEEEkeywords}

Security, cyberattacks, performance evaluation.
\end{IEEEkeywords}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

While security is a constant concern in modern computer systems, the
amount of services a typical application relies on and offers, and the
sheer quantity of services and microservices, modern cloud-native,
applications are composed of, makes extremely complicated to create
configurations, for every one of them, that are at the same time
secure and performant.

The wide variety of attacks and attack techniques also makes it difficult
to create a single, static defense that can deflect every
possible attack an interested third party might mount. Defense needs
then to adapt to be able to confuse, deflect or avoid this kind of
attacks. As an example, we can simply imagine that the name or IP of a node
in a service is constantly changing; the attacker will be unable to
use stored information (such as vulnerabilities) from that particular
node to, later on, scale privileges, extract information (exfiltrate)
from the net, or simply check if that service is up. Fortunately,
modern cloud-native deployments facilitate that kind of defense: since
the whole deployment is software defined, we can embed these changes
within the deployment instructions themselves.

The kind of defense tecnhique that tries to present a variable target
to possible attackers is called {\em moving target defense} or
MTD. The concept was proposed initially by the Federal Networking and
Information Technology Research and Development (NITRD) Program for the first time in 2009
\cite{moving-target}, and presented in a series of documents
\cite{nitrd} and books that bind the papers presented in the first
symposium dedicated to the topic \cite{jajodia2011moving}. The moving
target defense \cite{Cai2016,ward2018survey,lei2018moving} does not specify either the kind of attack that defense
is being put up against, which could be from privilege scalation to
denial of service attacks, the service that is being hardened or secured
using this technique, which can go from a web or proxy server to a
software defined network \cite{Makanju:2017:ECM:3067695.3075604}, or
the kind of technique that is used to generate a moving target, which
can also be simple randomization \cite{gallagher_morpheus:_2019} of
the user-facing information through churn, that is, changing often
from a set of pre-established configurations through more ellaborate
systems like evolutionary algorithms \cite{john_evolutionary_2014} that, at the same time, optimize
security or some other measure, like performance. % Performance of what? less overhead?
% The above could be easier to understand if we say first:
% ... does not specify either the kind of attack, the service is protecting or the technique ..
% and after that we give the examples. - Mario
% Good call, but a bit late for that... - JJ


Our previous paper \cite{erseco:evostar} was a proof of concept and
tested the framework we have created for evolving a set of
configurations that can be used in a MTD policy. Our target was
hardening {\sf nginx} installations and we used as a fitness function
{\em Zed Attack Proxy} (ZAP), an open source tool that gives as a score for an installation the
number of {\em alerts}, or possible security vulnerabilities, it
raises. We tested different configurations and found that evolutionary
algorithms are able to generate configurations with a low score (lower is
better), and also that every execution of the algorithm yields several
configurations with the same fitness, which can then be used straight
away to change the configuration of the server.

However, that was intended as an initial exploration of the concept of
using evolutionary algorithms to generate low-vulnerability and
diverse nginx configurations. We needed to
explore the possibilities of the evolutionary algorithm further, by
tuning its parameters so that better
configurations can be found with less evaluations. Also, we needed to explore
different possibilities of the scoring tool, to check which mode would
be better for the MTD task. These will be the two main objectives of
this paper.

The rest of the paper is organized as follows: next we will present
the state of the art in evolutionary methods applied to MTD; the next
Section \ref{sec:met} will present the methodology used in this paper,
followed by the experimental results, which we will discuss next,
finishing with our conclusions and future lines of work.

\section{State of the art}
\label{sec:soa}


MTD was proposed by the first time in 2009 \cite{moving-target} by an
organism called NITRD as part of an officially sponsored research
program to improve the cyberdefense skills in the United States. MTD is targeted % America -> USA? - Mario
% right, fixed - JJ
towards making what is called the attack surface
\cite{manadhata2011formal}, that is, the different mechanisms by which
the attacker would be able to gain access, unpredictable
\cite{jajodia2011moving}, and thus rendering attacks against it either too
expensive or too complex to pursue, possibly forcing the attacker to
choose some other, more affordable, place. For instance, an attacker
analyzing byte patterns coming from different nodes such as the one
described in \cite{piskozub2019resilience} will find those patterns
disrupted, and so profiling of specific nodes impossible.


This program was pursued using different kind of techniques, of which
a good initial survey was made in \cite{Cai2016}, reexamined in
\cite{Larsen201428} and more recently in
\cite{lei2018moving,ward2018survey,cho2019toward}. MTD is used as a
defense as well as detection technique
\cite{tian2019moving,POTTEIGER2020102954}; for instance, it can be
used to deflect distributed denial of service attacks
\cite{prathyusha2020review}; besides, it has been proved effective
against exfiltration techniques via the use of an open source
framework called MoonRaker \cite{shade2020moonraker} or to protect
software defined networks \cite{al2011toward}. Several techniques have
been applied recently; for instance, natural randomization in services
can be enhanced \cite{kansal2020improving}; or, beyond the technique
that is used, deep reinforcement learning can try and find the best
moment for changing configurations \cite{eghtesad2019deep}, a topic
that is normally left behind. These techniques have been surveyed in
\cite{Zheng2019207,cho2019toward}, to which we direct the interested
reader.

However, in this paper we focus on those that use evolutionary
algorithms as a method of optimization as well as generation of new
configurations; evolutionary algorithms are no strangers in the
cybersecurity world, and in fact, since early on, they were applied to intrusion
detection systems \cite{WU20101}. It was only natural that they were
also applied, since the inception of the technique, to MTD. An
evolutionary-like bioinspired algorithm  called {\em
  symbiotic embedded machines} (SEM) was proposed by Cui and Stolfo
\cite{cui2011symbiotes} as a methodology for {\em injecting} code into
systems that would behave in a way that would be similar to a
symbiotically-induced immune system. Besides that principled
biological inspiration, SEMs used mutation as a mechanism for avoiding
signature based detection methods and thus become a MTD system.

Other early MTD solutions included the use of rotating virtual webservers
\cite{huang2011introducing}, every one with a different attack
surface, to avoid predictability and achieve the variable attack
surface that was being sought. However, while this was a practical and
actionable kind of defense,
no specific technique was proposed to individually configure every
virtual server, proposing instead manual configuration of web servers
(such as nginx and Apache), combined with plug-ins (some of which do
not actually work together). A similar technique, taken to the cloud, was proposed
by Peng et al. \cite{peng2014moving}:  a specific
mechanism that uses different cloud instances and procedures for moving
virtual machines between them; still, no other
mechanism was proposed to establish these configurations, which were
simply left to being designed by hand, as long as there were enough of them.

Bioinspired solutions filled that gap: after the early {\em bioinspired} approaches to MTD, explicit
methodologies that used evolutionary algorithms were conceptually described for the first
time by Crouse and Fulp in \cite{6111663}. This was intended mainly as
a proof of concept, and describes 80 parameters, of which just half
are evolved. The GA minimizes the number of vulnerabilities, but the
study also emphasizes the degree of diversity achieved by successive
generations in the GA, which impact on the diversity needed by the
MTD. Lucas et al. in \cite{lucas2014initial} applied those theoretical
concepts to a framework called EAMT, a Python-based system that uses
evolutionary algorithms to create new configurations, which are then
implemented in a virtual machine and scored using scanning tools such
as Nessus. Later on, John et
al. \cite{john_evolutionary_2014} make a more explicit and practical
use of an evolutionary algorithm, describing a host-level
defense system, that is, one that operates at the level of a single
node in the network, not network-wide, and works on the configuration
of the Apache server, evolving them and evaluating at the parameter
level using the above mentioned CVSS score. These two systems
highlighted the need for, first, a practical way of applying the MTD
to an actual system, to the point of implementing it in a real virtual
machine, and second, the problematic of scoring the generated
configurations. In the next section we will explain our proposed
solutions to these two problems.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology, experimental setup and results}
\label{sec:met}

% 1. Indicate why nginx has been chosen, refer to other papers and what kind of services they chose and why this is new. This must include also a brief explanation of tables 1 and 2 and what the parameters mean, as well as the possible influence on vulnerability they might have.

As in our previous paper \cite{\cite{erseco:evostar}}, we have chosen
{\sf nginx}; it's a very popular static web server, which is also used
as an inverse proxy, API gateway and load balancer. Latest versions
of{\sf nginx} (1.17.x) have more than 700 configuration
directives. As a matter of fact, {\sf nginx} has been the target of
optimization by evolutionary algorithms recently \cite{chi2018hybrid},
but this is not the main focus of our work presently.


These directives affect in different ways the behavior of
the web site (or service) that is behind it, or simply change the
values in the response headers; we will show the ones we will working
with next. The following Subsection \ref{subs:setup}
will outline the setup actually used for running the experiments, and
results will be presented last in Subsection \ref{subs:results}.

\subsection{Description of the attack surface parameters}

There is a huge number parameters that could potentially be chosen for
our experiments so to validate our hypothesis we choose a subset of 9
{\sf nginx} directives (\ref{table:nginx_directives}) and 6 HTTP headers
(\ref{table:http_headers}), all of them related to security
hardening. The subset is extracted from the DISA STIG recommendations
for hardening webservers based in the CVSS score. Most of this values
are defined as Apache HTTP server configuration values but have a
{\sf nginx} equivalent directive.

\begin{table}
\centering
\begin{tabular}{|l|l|c|}
\hline
\textbf{STIG ID} & \textbf{Directive name} 	   & \textbf{Possible values} \\ \hline
V-13730 & worker\_connections            & 512 - 2048 \\ \hline
V-13726 & keepalive\_timeout             & 10 - 120 \\ \hline
V-13732 & disable\_symlinks              & True/False \\ \hline
V-13735 & autoindex                      & True/False \\ \hline
V-13724 & send\_timeout                  & True/False \\ \hline
V-13738 & large\_client\_header\_buffers & 512 - 2048 \\ \hline
V-13736 & client\_max\_body\_size        & 512 - 2048 \\ \hline
V-6724  & server\_tokens                 & True/False \\ \hline
        & gzip                           & True/False \\ \hline
\end{tabular}
\label{table:nginx_directives}
\caption{List of {\sf nginx} directives whose value is evolved in this
work}
\end{table}
%

These are the directives that have been used in this paper; their
equivalent STIG ID is shown in Table
\ref{table:nginx_directives}.
% It would be convenient to rewrite most of these directives.
\begin{itemize}
\item \texttt{worker\_connections}: Maximum number of simultaneous connections that can be opened by an {\sf nginx} process.
\item \texttt{keepalive\_timeout}: Period during which a server-side client
  connection will remain open  before timing out.
\item \texttt{disable\_symlinks}: Restricts opening files that are
  actually symbolic links. When set to off (the default), if  some component of the
  path is a symbolic link the access to that file is denied. An
  additional value, {\tt if\_not\_owner}, is not considered in this study.
\item \texttt{autoindex}: When activated it shows the contents of the directories, otherwise it does not show anything.
\item \texttt{send\_timeout}: sets a value for the waiting time to
  transmit a response to the client; the client will close the
  connection if nothing is received during that time. It is set by
  default to 60s, we will disable it completely (setting it to 0) or
  set it to 1 second.
\item \texttt{large\_client\_header\_buffers}: This directive has two
  values: the number of buffers that will read large client request
  headers, and their size. We just evolve the second number, while the
  first one is set to 4.

\item
\texttt{client\_max\_body\_size}: Maximum allowed size of the client request body, specified in the {\tt
  Content-Length} field of the request header. The value will indicate
the number of Megabytes allowed.
\item
\texttt{server\_tokens}: Enable or disable the broadcast of the {\sf
  nginx} version on the error pages and in the `Server' response
header. It's on by default, and we admit on or off values, although the
directive can also use a {\em build} value and simply a string.
\item
\texttt{gzip}: This directive enables or disables compression of HTTP
responses. This directive doesn't affect directly the security but
adds entropy to the different generated configurations; it also
affects the performance of the server and its throughput. You can
additionaly use {\tt gzip\_min\_length} to establish the length under
which there will be no compression.
\end{itemize}

\begin{table}

  \centering
  \caption{Selected list of directives affecting HTTP headers, and the
  values that we are using in this paper.}
  \label{table:http_headers}

\begin{tabular}{|l|l|}
\hline
\textbf{Header name}           & \textbf{Possible values} \\ \hline
X-Frame-Options                & \shortstack[l]{SAMEORIGIN \\
  ALLOW-FROM \\ DENY \\ WRONG VALUE} \\ \hline
X-Powered-By                   & \shortstack[l]{PHP/5.3.3 \\ PHP/5.6.8 \\ PHP/7.2.1 \\ Django2.2 \\ nginx/1.16.0} \\ \hline
X-Content-Type-Options         & nosniff \\ \hline
Server                         & \shortstack[l]{apache \\ caddy \\ nginx/1.16.0} \\ \hline
X-XSS-Protection	           & \shortstack[l]{0 \\ 1 \\ 1; mode=block} \\ \hline
Content-Security-Policy		   & \shortstack[l]{default-src 'self' \\ default-src 'none' \\ default-src 'host *.google.com'} \\ \hline
\end{tabular}

\end{table}
%
The web servers also send a number of headers, which can be configured
also. These are presented next, with possible values represented in
Table \ref{table:http_headers}.
\begin{itemize}
\item
\texttt{X-Frame-Options}: The header that has the same name as this
directive, when set, bans browsers rendering of frame-embedded pages. Web servers can use it to prevent what are called
\textit{clickjacking} attacks on their pages, that is, overlaying a
transparent frame over a site to make users click on the transparent
layer believing they are clicking on the one below. Several values are
possible: only allow if the embedding page belongs to the same site,
for instance, but you can also directly ban it. We have 4 different
options for this, including a value that's intentionally wrong.
\item
\texttt{X-Powered-By}: This is a string that is supposed to tell the
name and versions of the application that generated the response. It
is generally recommended not giving too extensive information in this
header because can reveal details that can facilitate the task of
finding and exploiting security flaws for specific versions just
looking up in an online database. Setting different values do not affect directly to the
security by itself but adds entropy to the generated configurations.
\item
\texttt{X-Content-Type-Options}: This directive controls the opration
of the homonym HTTP response header; when set,  changing the \textit{MIME} types
announced in the `Content-Type' is disabled, which is used to avoid
`MIME type sniffing' attacks. % Add something about these attacks - JJ
\item
\texttt{server}: This directive is related to the {\tt Server} HTTP
header, which is used to communicate to browsers metadata about the
server software used by the application. This can be as informative or
as misleading as we want; as a matter of fact, it is a good practice
not to give too
extensive information of software versions, but we can cheat the
attacker telling wrong server version info. Doesn't affect directly to
the security but adds entropy to the generated configurations. % Difference with X-Powered-By? - JJ
\item
\texttt{X-XSS-Protection}: This directive is used to set the value of
the corresponding HTTP response header; as the rest of these server
directives, it's interpreted by browsers and used by them to avoid
loading pages when they detect reflected cross-site
scripting (XSS) attacks. % Need to clarify this.
\item
\texttt{Content-Security-Policy}: we can set this directive to
different values to avoid the load of certain kind of content. We have
three different values for this directive, allowing content only if
it's loaded from the same page ({\tt self}), loading from nowhere
({\tt none}), or including google domains {\tt hots:*.google.com}, in
this case mainly for user tracking. These values are shown in Table
\end{itemize}

% Say something about how every one of these directives has a value
% that is generally more secure than others, or at least corresponds
% to better practices, but in some cases it covers cases that are not
% really an issue in the specific application - JJ

\subsection{Experimental setup}
\label{subs:setup}
% 2. Proceed to the evolutionary algorithm used
To write the genetic algorithm we have chosen the Python programming
language due to the availability of the OWASP ZAP API in that
language. In addition, although this project does not require high
performance, several publications indicate a very good performance of
the Python language when working with genetic algorithms
\cite{merelo-guervos_comparison_2016}. The implementation has been
written for this project, and is a simple implementation of a
canonical genetic algorithm; this has been released as free software
together with the rest of the framework. The genetic algorithm works
generating a population of $n$ individuals. Each individual is a
chromosome of 15 gens, each gen referring to the {\sf nginx} directive
or HTTP security headers shown in the previous subsection.

After generating the population we calculate the fitness of that
population using OWASP ZAP, which gives a scalar value with the number of
known vulnerabilities a configuration has. The OWASP ZAP Python API
calling a container with the Docker version of OWASP ZAP; this simulates a
real environment using the {\tt example.com} domain and the generated
configuration. This API will yield the mentioned
scalar value depending on the number of known vulnerabilities found
for that configuration.

Once every individual has been assigned a fitness, we sort the population list
in reverse order to set the better ones at the end of the list and
get the $p$ (pressure) values that we will evolve using mutation and
crossover. This will be repeated  during 15 generations.

For evolving the configuration we have written two different crossover
functions that use either one or two points. We will run the
experiments for each function to find out which
one gives better results \cite{LNCS2439:ID186:pp142}.

Also, we are mutating the population with a chance of 0.4 using two
different mutation methods. One changing random gen with a random
correct value or increasing/decreasing random gen.
After the mutation, we calculate the fitness of the new element, sort
the population and run again the algorithm until no more generations
left.

% 4. Detail the cloud infrastructure that has been created for this.

OWASP ZAP is a heavy-weight process taking a certain amount of time to analyze
each web configuration so we ran the experiments in three AWS EC2 t3.medium
instances all running Ubuntu 18.04 LTS with Docker installed, each
instance has 2 vCPU and 4 GiB of RAM. Each instance runs a different
set of experiments of 16, 32 and 64 population size. To orchestrate
the instances we used a simple Ansible playbook.

For the 16 individuals population size the experiment took an average
of 35 minutes, taking 80 minutes in the 32 individuals population size
and 180 minutes for the 64 individuals population size. This times are
the reason of running each population size in different EC2
instances. The running time of all instances was 266 hours, equivalent
to 11 days of total processing time, having a total cost of \$11.07.

A set of experiments has been carried out with the static site and the juice shop. These have been the parameters that have varied \begin{itemize}
\item Mutation is either {\sf random} or {\sf one}. In the first case, the selected configuration variable is changed by another random value. In the second case, one is added or subtracted from its value.
\item Crossover uses either one or two points.
\item Population goes from 16 to 64 in the case of the static web site, it stops at 32 in the juice shop.
\item The evolutionary algorithm is run for 15 generations.
\end{itemize}

In this case, it's difficult to know in advance what would be the
correct configuration for the evolutionary algorithm, so all these
options have been tested and evaluated to check its influence in the
eventual result. Experiments have been repeated, for each
configuration, 15 times.

\subsection{Experimental results}
\label{subs:results}

\section{Conclusions and discussion}
\label{sec:conclusions}

After checking the results we can state that genetic algorithms can
help us to improve the security of a system by generating many
different low-vulnerability configurations for a real, and industry
standard, server, thus being suitable to carry out the moving target
defense along with passive or reactive policies of service
configuration change. An evolutionary algorithm was successfully
applied, allowing configurations to evolve diversely and securely,
although there is a trade-off between them, with lower vulnerability
configurations being generated in less quantity than others with a
slightly higher vulnerability degree. However, low vulnerability
configurations are consistently generated, which means that we could
extract from a population different configurations with different
degrees of vulnerability, contributing even more to the entropy of the
system.

Some vulnerabilities can be caused by a bad configuration or by an
unfortunate combination of configurations that it is difficult for an
administrator to discover manually due to the large number of
parameters and possible combinations. Thanks to a genetic algorithm it
was possible to find more secure configurations. The configurations
were represented as chromosomes and the algorithm took those
chromosomes through a series of selection, crossover and mutation
operators
 that resulted in  configurations  that were safer than the previous
generation. Using these evolved configurations we achieve the main
objective of this paper, which is transforming our server into a
moving target by changing the configuration with a reasonable
periodicity (with a lower bound of approximately 4-5 hours) using the
configuration with the lowest (or second-lowest) generated by the
algorithm.

These results open new and promising new lines of work. Focusing on
the improvement of {\sf nginx},  more directives as
well as more security-related HTTP headers can be added. This will
expand the search space of the evolutionary algorithm, and this can be
a problem, which is why another possible future work would be to
improve the genetic algorithm avoiding erroneous individuals in the
initial population, generating in this way a safer population, and
besides include some program of benchmark in our fitness function to
know that besides safe, our configuration has a good performance.

However, one of the key issues is speed. The number of evaluations we
are able to use in our evolutionary algorithm is relatively small for
EA standards. We would need to speed up evaluation, and since it
relies on an external tool, the only possible way is to use parallel
evaluation by replicating the docker containers being tested and
having ZAP score them at the same time. A small (4-fold, in the case
of the cloud instance used in our problem) could be achieved this
way. However, much better improvements could be achieved by using
surrogate models \cite{ong2003evolutionary}. This would mean training
some machine learning model that is able to immediately issue a score
for a certain configuration level. These surrogate models could be
combined with real evaluations to give an accurate result, and be able
to reach a good number of evaluations.

Finally, the algorithm itself can be improved, by testing different
types of selection procedures, and tuning its greediness. This is
something that can be done immediately, and will be one of our next steps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}

This paper has been supported in part by projects DeepBio (TIN2017-85727-C4-2-P).

\bibliographystyle{IEEEtran}
\bibliography{geneura,moving-target}

\end{document}
